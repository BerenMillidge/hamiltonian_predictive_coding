{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hamiltonian_predictive_coding_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCAx-D0ia7-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" imports \"\"\"\n",
        "#Hamiltonian Predictive Coding. Merges MCMC sampling with Predictive coding networks to in theory allow for a biologically plausible way for the brain to achieve MCMC sampling\n",
        "#with any arbitrary function. The HMC steps are not actually unbiologically plausible at all, and can explain both neural oscillations and inhibitory cell populations beyond that required for error units.\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw13wh0heQrC",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkP3wmc3alYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" functions \"\"\"\n",
        "\n",
        "def set_tensor(tensor):\n",
        "    return tensor.to(DEVICE).float()\n",
        "\n",
        "\n",
        "def tanh(xs):\n",
        "    return torch.tanh(xs)\n",
        "\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def tanh_deriv(xs):\n",
        "    return 1.0 - torch.tanh(xs) ** 2.0\n",
        "\n",
        "\n",
        "def linear_deriv(x):\n",
        "    return set_tensor(torch.ones((1,)))\n",
        "\n",
        "\n",
        "def onehot(x):\n",
        "    z = np.zeros([10])\n",
        "    z[x] = 1.0\n",
        "    return z\n",
        "\n",
        "\n",
        "def get_batch_size(x_batch=None, y_batch=None):\n",
        "    \"\"\" torch \"\"\"\n",
        "    if x_batch is not None:\n",
        "        return x_batch.size()[1]\n",
        "    elif y_batch is not None:\n",
        "        return y_batch.size()[1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def flatten_array(array):\n",
        "    return torch.flatten(torch.cat(array))\n",
        "\n",
        "\n",
        "def classification_accuracy(pred_labels, true_labels):\n",
        "    correct = 0\n",
        "    batch_size = pred_labels.size()[1]\n",
        "    for b in range(batch_size):\n",
        "        if torch.argmax(pred_labels[:, b]) == torch.argmax(true_labels[:, b]):\n",
        "            correct += 1\n",
        "    return correct / batch_size\n",
        "\n",
        "\n",
        "def get_img_list(dataset, n_batches, batch_size):\n",
        "    arr = [\n",
        "        np.array(\n",
        "            [\n",
        "                np.array(dataset[(n * batch_size) + i][0]).reshape([784, 1]) / 255.0\n",
        "                for i in range(batch_size)\n",
        "            ]\n",
        "        ).T.reshape([784, batch_size])\n",
        "        for n in range(n_batches)\n",
        "    ]\n",
        "    return [set_tensor(torch.from_numpy(d)) for d in arr]\n",
        "\n",
        "\n",
        "def get_label_list(dataset, n_batches, batch_size):\n",
        "    arr = [\n",
        "        np.array([onehot(dataset[(n * batch_size) + i][1]) for i in range(batch_size)]).T\n",
        "        for n in range(n_batches)\n",
        "    ]\n",
        "    return [set_tensor(torch.from_numpy(d)) for d in arr]\n",
        "\n",
        "\n",
        "def plot_images(imgs):\n",
        "    imgs = [np.reshape(imgs[:, i], [28, 28]) for i in range(imgs.shape[1])]\n",
        "    _, axes = plt.subplots(2, 5)\n",
        "    axes = axes.flatten()\n",
        "    for i, img in enumerate(imgs):\n",
        "        axes[i].imshow(img, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "def symplectic_leapfrog_integrator(mus, momenta, dFdmu, path_len, step_size):\n",
        "    #the mus and momenta should be batch x feature_size. The momenta are sampled from a gaussian distribution #dfdmu is the gaussian update\n",
        "    #clone mus and momenta as changing them in the integration step\n",
        "    mus,momenta = mus.clone(), momenta.clone()\n",
        "    #take the initial step\n",
        "    momenta -= step_size * dFdmu / 2  # half step\n",
        "    #print(\"symplectic integration steps: \", int(path_len / step_size) - 1)\n",
        "    for _ in range(int(path_len)):\n",
        "        #interweave updates for conservative symplectic dynamics\n",
        "        mus -= step_size * momenta  # whole step\n",
        "        momenta -= step_size * dFdmu  # whole step\n",
        "    mus -= step_size * momenta  # whole step\n",
        "    momenta -= step_size * dFdmu / 2  # half step\n",
        "    # momentum flip at the end\n",
        "    return mus, -momenta\n",
        "\n",
        "def MH_acceptance(F_init, F_end):\n",
        "  #computes the MH acceptance step in parallel for a batch. Returns a boolean vector \n",
        "  #which should determine whether the origin sample is accepted (true) or rejected (false)\n",
        "  #Interestingly MH acceptance step is determined by the free-energies, which is a nice mathematical connection\n",
        "  thresh = torch.min(torch.ones_like(F_init), torch.exp(F_init - F_end))\n",
        "  rand = set_tensor(torch.rand(*F_init.shape))\n",
        "  return rand <= thresh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaSc0yP0eOEl",
        "colab_type": "text"
      },
      "source": [
        "# Standard Predictive Coding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpKuPzEpbA8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" layers \"\"\"\n",
        "\n",
        "class PredictiveCodingLayer(object):\n",
        "    def __init__(self, input_size, output_size, learning_rate, fn, fn_deriv):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fn = fn\n",
        "        self.fn_deriv = fn_deriv\n",
        "        weights = torch.empty((input_size, output_size)).normal_(mean=0.,std=0.1)\n",
        "        self.weights = set_tensor(weights)\n",
        "        self.mu = None\n",
        "\n",
        "    def reset_mu(self, batch_size):\n",
        "        mu = torch.empty((self.output_size, batch_size)).normal_(mean=0.,std=1.)\n",
        "        self.mu = set_tensor(mu)\n",
        "\n",
        "    def predict(self):\n",
        "        return self.fn(torch.matmul(self.weights, self.mu))\n",
        "\n",
        "    def update_mu(self, prior_err, likelihood_err):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, self.mu))\n",
        "        delta = torch.matmul(self.weights.T, likelihood_err * fn_deriv)\n",
        "        delta = -prior_err + delta\n",
        "        self.mu += self.learning_rate * delta\n",
        "\n",
        "    def update_weights(self, pred_err):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, self.mu))\n",
        "        delta = torch.matmul(pred_err * fn_deriv, self.mu.T)\n",
        "        self.weights += self.learning_rate * delta\n",
        "\n",
        "class AmortisedLayer(object):\n",
        "    def __init__(self, input_size, output_size, learning_rate, fn, fn_deriv):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fn = fn\n",
        "        self.fn_deriv = fn_deriv\n",
        "        weights = torch.empty((input_size, output_size)).normal_(mean=0.,std=0.1)\n",
        "        self.weights = set_tensor(weights)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.fn(torch.matmul(self.weights, state))\n",
        "\n",
        "    def update_weights(self, pred_err, state):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, state))\n",
        "        delta = torch.matmul(pred_err * fn_deriv, state.T)\n",
        "        self.weights += self.learning_rate * delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf-priyxeMMX",
        "colab_type": "text"
      },
      "source": [
        "# Hamiltonian Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM_zjlTadt34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" layers \"\"\"\n",
        "\n",
        "class HamiltonianPredictiveCodingLayer(object):\n",
        "    def __init__(self, input_size, output_size, learning_rate, fn, fn_deriv,N_HMC_samples, integrator_step_size, integrator_path_len,mass_metric_scalar):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fn = fn\n",
        "        self.fn_deriv = fn_deriv\n",
        "        weights = torch.empty((input_size, output_size)).normal_(mean=0.,std=0.1)\n",
        "        self.weights = set_tensor(weights)\n",
        "        self.N_HMC_samples = N_HMC_samples\n",
        "        self.integrator_step_size = integrator_step_size\n",
        "        self.integrator_path_len = integrator_path_len\n",
        "        self.mass_metric_scalar = mass_metric_scalar\n",
        "        self.mu = None\n",
        "\n",
        "    def reset_mu(self, batch_size):\n",
        "        mu = torch.empty((self.output_size, batch_size)).normal_(mean=0.,std=1.)\n",
        "        self.mu = set_tensor(mu)\n",
        "\n",
        "    def predict(self):\n",
        "        return self.fn(torch.matmul(self.weights, self.mu))\n",
        "\n",
        "    def update_mu(self, prior_err, likelihood_err):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, self.mu))\n",
        "        delta = torch.matmul(self.weights.T, likelihood_err * fn_deriv)\n",
        "        delta = -prior_err + delta\n",
        "        self.mu += self.learning_rate * delta\n",
        "\n",
        "    def update_weights(self, pred_err):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, self.mu))\n",
        "        delta = torch.matmul(pred_err * fn_deriv, self.mu.T)\n",
        "        self.weights += self.learning_rate * delta\n",
        "\n",
        "    def predict_with_mu(self,mu):\n",
        "      return self.fn(torch.matmul(self.weights, mu))\n",
        "\n",
        "    def predict_deriv(self,mu):\n",
        "      return self.fn_deriv(torch.matmul(self.weights, mu))\n",
        "\n",
        "\n",
        "    def PC_HMC_sample_step(self, prev_sample,pred_above,mu_below):\n",
        "      #compute dF/dmu gradients\n",
        "      prior_err = prev_sample - pred_above #assumed to be fixed here\n",
        "      likelihood_err= mu_below - self.predict_with_mu(prev_sample)\n",
        "      fn_deriv = self.fn_deriv(torch.matmul(self.weights, prev_sample))\n",
        "      likelihood_back = torch.matmul(self.weights.T, likelihood_err * fn_deriv)\n",
        "      dFdmu = -prior_err + likelihood_back\n",
        "      #sample random \n",
        "      momenta = set_tensor(torch.empty(prev_sample.shape).normal_(mean=0.0,std=1) * self.mass_metric_scalar)\n",
        "      new_samp, new_momenta = symplectic_leapfrog_integrator(prev_sample,momenta,dFdmu,self.integrator_path_len,self.integrator_step_size)\n",
        "      #compute layer-wise free energies for MH acceptance step\n",
        "      F_old = torch.sum(prior_err * prior_err,dim=0) + torch.sum(likelihood_err*likelihood_err,dim=0)\n",
        "      F_new = torch.sum((new_samp - pred_above)**2,dim=0) + torch.sum((mu_below - self.predict_with_mu(new_samp))**2,dim=0)\n",
        "      MH_bools = MH_acceptance(F_old,F_new)\n",
        "      rejected_idxs = torch.where(~MH_bools)[0] #reverse ordering so it's rejected. Tilde ~ is logical not -- never knew this!\n",
        "      new_samp[:,rejected_idxs] = prev_sample[:,rejected_idxs]\n",
        "      return new_samp, new_momenta\n",
        "    \n",
        "    def HMC_sampling(self,mu_below,pred_above):\n",
        "      samples = torch.zeros([self.N_HMC_samples, *self.mu.shape])\n",
        "      samples[0] = self.mu.clone()\n",
        "      for n in range(1,self.N_HMC_samples):\n",
        "        new_samp, new_momenta = self.PC_HMC_sample_step(samples[n-1],mu_below,pred_above)\n",
        "        samples[n,:,:] = new_samp\n",
        "      return samples\n",
        "\n",
        "class AmortisedLayer(object):\n",
        "    def __init__(self, input_size, output_size, learning_rate, fn, fn_deriv):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fn = fn\n",
        "        self.fn_deriv = fn_deriv\n",
        "        weights = torch.empty((input_size, output_size)).normal_(mean=0.,std=0.1)\n",
        "        self.weights = set_tensor(weights)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.fn(torch.matmul(self.weights, state))\n",
        "\n",
        "    def update_weights(self, pred_err, state):\n",
        "        fn_deriv = self.fn_deriv(torch.matmul(self.weights, state))\n",
        "        delta = torch.matmul(pred_err * fn_deriv, state.T)\n",
        "        self.weights += self.learning_rate * delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GUtn6meWqb",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEveJ0HvbJwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HamiltonianNetwork(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_sizes,\n",
        "        v_learning_rate,\n",
        "        q_learning_rate,\n",
        "        fn,\n",
        "        fn_deriv,\n",
        "        threshold,\n",
        "        N_HMC_samples,\n",
        "        integrator_step_size,\n",
        "        integrator_path_len,\n",
        "        mass_metric_scalar\n",
        "    ):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.v_learning_rate = v_learning_rate\n",
        "        self.q_learning_rate = q_learning_rate\n",
        "        self.n_infer_steps_train = n_infer_steps_train\n",
        "        self.n_infer_steps_test = n_infer_steps_test\n",
        "        self.fn = fn\n",
        "        self.fn_deriv = fn_deriv\n",
        "        self.n_activations = len(layer_sizes)\n",
        "        self.n_layers = len(layer_sizes) - 1\n",
        "        self.threshold = threshold\n",
        "        self.N_HMC_samples = N_HMC_samples,\n",
        "        self.integrator_step_size = integrator_step_size\n",
        "        self.integrator_path_len = integrator_path_len\n",
        "        self.mass_metric_scalar = mass_metric_scalar\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.v_layers = []\n",
        "        self.q_layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            self.v_layers.append(\n",
        "                HamiltonianPredictiveCodingLayer(\n",
        "                    input_size=self.layer_sizes[i],\n",
        "                    output_size=self.layer_sizes[i + 1],\n",
        "                    learning_rate=self.v_learning_rate,\n",
        "                    fn=self.fn,\n",
        "                    fn_deriv=self.fn_deriv,\n",
        "                    N_HMC_samples = self.N_HMC_samples,\n",
        "                    integrator_step_size = self.integrator_step_size,\n",
        "                    integrator_path_len = self.integrator_path_len,\n",
        "                    mass_metric_scalar=self.mass_metric_scalar\n",
        "                )\n",
        "            )\n",
        "            self.q_layers.append(\n",
        "                AmortisedLayer(\n",
        "                    input_size=self.layer_sizes[i + 1],\n",
        "                    output_size=self.layer_sizes[i],\n",
        "                    learning_rate=self.q_learning_rate,\n",
        "                    fn=self.fn,\n",
        "                    fn_deriv=self.fn_deriv,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def reset(self, batch_size):\n",
        "        self.v_preds = [[] for i in range(self.n_activations)]\n",
        "        self.v_pred_errs = [[] for i in range(self.n_activations)]\n",
        "        self.q_preds = [[] for i in range(self.n_activations)]\n",
        "        self.q_pred_errs = [[] for i in range(self.n_activations)]\n",
        "\n",
        "        for layer in self.v_layers:\n",
        "            layer.reset_mu(batch_size)\n",
        "            \n",
        "    def set_input(self, x_batch=None, y_batch=None):\n",
        "        if x_batch is not None:\n",
        "            self.q_preds[0] = x_batch.clone()\n",
        "        if y_batch is not None:\n",
        "            self.v_layers[-1].mu = y_batch.clone()\n",
        "\n",
        "    def infer(self, n_steps, x_batch=None, y_batch=None,use_sampling=False):\n",
        "        \"\"\"\n",
        "        Run inference with both amortised and variational networks \n",
        "        \"\"\"\n",
        "        batch_size = get_batch_size(x_batch=x_batch, y_batch=y_batch)\n",
        "        self.reset(batch_size)\n",
        "        self.set_input(x_batch, y_batch)\n",
        "        self.amortised_forward(set_mu=True)\n",
        "        self.variational_backward()\n",
        "        n_iterations = self.variational_updates(n_steps, x_batch,use_sampling=use_sampling)\n",
        "        return {'n_iterations': n_iterations}\n",
        "\n",
        "    def variational_infer(self, n_steps, x_batch=None, y_batch=None):\n",
        "        \"\"\"\n",
        "        Run inference with variational network\n",
        "        \"\"\"\n",
        "        batch_size = get_batch_size(x_batch=x_batch, y_batch=y_batch)\n",
        "        self.reset(batch_size)\n",
        "        self.set_input(x_batch, y_batch)\n",
        "        self.variational_backward()\n",
        "        n_iterations = self.variational_updates(n_steps, x_batch)\n",
        "        return n_iterations\n",
        "\n",
        "    def amortised_infer(self, x_batch):\n",
        "        \"\"\"\n",
        "        Run inference with amortised network\n",
        "        \"\"\"\n",
        "        batch_size = get_batch_size(x_batch=x_batch)\n",
        "        self.reset(batch_size)\n",
        "        self.set_input(x_batch)\n",
        "        self.amortised_forward(set_mu=False)\n",
        "\n",
        "    def amortised_forward(self, set_mu=False):\n",
        "        \"\"\"\n",
        "        Forward pass of amortised network\n",
        "        \"\"\"\n",
        "        for i in range(self.n_layers):\n",
        "            self.q_preds[i + 1] = self.q_layers[i].predict(self.q_preds[i])\n",
        "            if set_mu:\n",
        "                self.v_layers[i].mu = self.q_preds[i + 1].clone()\n",
        "\n",
        "    def variational_backward(self):\n",
        "        \"\"\"\n",
        "        Backward pass of variational network\n",
        "        \"\"\"\n",
        "        for i in reversed(range(self.n_layers)):\n",
        "            self.v_preds[i] = self.v_layers[i].predict()\n",
        "\n",
        "    def variational_updates(self, n_steps, x_batch=None,use_sampling=False):\n",
        "        \"\"\"\n",
        "        Variational updates\n",
        "        \"\"\"\n",
        "        n_iterations = 0\n",
        "        for _ in range(n_steps):\n",
        "            zeros = set_tensor(torch.zeros_like(self.v_preds[0]))\n",
        "            data_err = (zeros if x_batch is None else (x_batch - self.v_preds[0]))\n",
        "            self.v_pred_errs[0] = data_err\n",
        "            self.v_preds[-1] = set_tensor(torch.zeros_like(self.v_layers[-1].mu))\n",
        "\n",
        "            for i in range(self.n_layers):\n",
        "                self.v_pred_errs[i + 1] = self.v_layers[i].mu - self.v_preds[i + 1]\n",
        "                likelihood_err = self.v_pred_errs[i]\n",
        "                prior_err = self.v_pred_errs[i + 1]\n",
        "                if i == self.n_layers - 1:\n",
        "                    prior_err = set_tensor(torch.zeros_like(self.v_layers[i].mu))\n",
        "                if use_sampling==True:\n",
        "                  if i == 0:\n",
        "                    mu_below = x_batch\n",
        "                  else:\n",
        "                    mu_below = self.v_layers[i-1].mu\n",
        "                  new_mu, new_momenta = self.v_layers[i].PC_HMC_sample_step(self.v_layers[i].mu, self.v_preds[i+1],mu_below)\n",
        "                  self.v_layers[i].mu =  new_mu.clone()\n",
        "                else:\n",
        "                  self.v_layers[i].update_mu(prior_err, likelihood_err)\n",
        "                self.v_preds[i] = self.v_layers[i].predict()\n",
        "\n",
        "            n_iterations += 1\n",
        "            flat_array = flatten_array(self.v_pred_errs)\n",
        "            if torch.abs(flat_array).mean() < self.threshold:\n",
        "                break\n",
        "\n",
        "        return n_iterations\n",
        "\n",
        "    def variational_learn(self):\n",
        "        \"\"\"\n",
        "        Update weights of variational network\n",
        "        \"\"\"\n",
        "        for i in range(self.n_layers):\n",
        "            self.v_layers[i].update_weights(self.v_pred_errs[i])\n",
        "\n",
        "    def amortised_learn(self):\n",
        "        \"\"\"\n",
        "        Update weights of amortised network\n",
        "        \"\"\"\n",
        "        for i in range(self.n_layers):\n",
        "            self.q_pred_errs[i] = self.v_layers[i].mu - self.q_preds[i + 1]\n",
        "            state = self.q_preds[0] if i == 0 else self.v_layers[i - 1].mu\n",
        "            self.q_layers[i].update_weights(self.q_pred_errs[i], state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2BbPKbGbMPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" network functions \"\"\"\n",
        "\n",
        "def test_accuracy(model, x_list, y_list, n_infer_steps,use_sampling=False):\n",
        "    \"\"\"\n",
        "    Test accuracy of full model\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for (x_batch, y_batch) in zip(x_list, y_list):\n",
        "        model.infer(n_infer_steps, x_batch=x_batch,use_sampling=use_sampling)\n",
        "        pred_y = model.v_layers[-1].mu\n",
        "        accuracy += classification_accuracy(pred_y, y_batch)\n",
        "    return accuracy / len(x_list)\n",
        "\n",
        "def test_variational_accuracy(model, x_list, y_list, n_infer_steps):\n",
        "    \"\"\"\n",
        "    Test accuracy of variational network\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for (x_batch, y_batch) in zip(x_list, y_list):\n",
        "        model.variational_infer(n_infer_steps_test, x_batch=x_batch)\n",
        "        pred_y = model.v_layers[-1].mu\n",
        "        accuracy += classification_accuracy(pred_y, y_batch)\n",
        "    return accuracy / len(x_list)\n",
        "\n",
        "\n",
        "def test_amortised_accuracy(model, x_list, y_list):\n",
        "    \"\"\"\n",
        "    Test accuracy of amortised network\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for (x_batch, y_batch) in zip(x_list, y_list):\n",
        "        model.amortised_infer(x_batch)\n",
        "        pred_y = model.q_preds[-1]\n",
        "        accuracy += classification_accuracy(pred_y, y_batch)\n",
        "    return accuracy / len(x_list)\n",
        "\n",
        "def train_batch(model, x_batch, y_batch, n_infer_steps,use_sampling=False):\n",
        "    \"\"\"\n",
        "    Train network on single batch of data\n",
        "    \"\"\"\n",
        "    info = model.infer(n_infer_steps, x_batch=x_batch, y_batch=y_batch,use_sampling=use_sampling)\n",
        "    model.v_layers[-1].mu = y_batch.clone()\n",
        "    model.v_preds[-1] = y_batch.clone()\n",
        "    model.variational_learn()\n",
        "    #model.amortised_learn()\n",
        "    return info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6FUJQaxeq5n",
        "colab_type": "code",
        "outputId": "3aa620d4-9b91-48a0-8bdc-f6143d03b3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" run experiment \"\"\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 128\n",
        "#n_batches = 400\n",
        "n_batches = 5\n",
        "n_test_batches = 10\n",
        "\n",
        "n_infer_steps_train = 100\n",
        "n_infer_steps_test = 1000\n",
        "\n",
        "v_learning_rate = 0.005\n",
        "q_learing_rate = 0.001\n",
        "layer_sizes = [784, 300, 100, 10]\n",
        "\n",
        "#threshold = 0.15\n",
        "threshold = 0.01\n",
        "test_every = 1\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=True)\n",
        "test_set = torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=False)\n",
        "\n",
        "img_list = get_img_list(train_set, n_batches, batch_size)\n",
        "label_list = get_label_list(train_set, n_batches, batch_size)\n",
        "\n",
        "test_img_list = get_img_list(test_set, n_test_batches, batch_size)\n",
        "test_label_list = get_label_list(test_set, n_test_batches, batch_size)\n",
        "\n",
        "model = HamiltonianNetwork(layer_sizes=layer_sizes,\n",
        "                v_learning_rate=v_learning_rate,\n",
        "                q_learning_rate=q_learing_rate,\n",
        "                fn=tanh,\n",
        "                fn_deriv=tanh_deriv,\n",
        "                threshold=threshold,\n",
        "                N_HMC_samples=1000,\n",
        "                integrator_step_size=0.001,\n",
        "                integrator_path_len=100,\n",
        "                mass_metric_scalar=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_iterations = []\n",
        "    accuracy = []\n",
        "    q_accuracy = []\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"> Training Epoch {epoch}\")\n",
        "        for (x_batch, y_batch) in zip(img_list, label_list):\n",
        "            info = train_batch(model, x_batch, y_batch, n_infer_steps_train)\n",
        "            n_iterations.append(info['n_iterations'])\n",
        "\n",
        "        if epoch % test_every == 0:\n",
        "            ep_acc = test_accuracy(model, test_img_list, test_label_list, n_infer_steps_test,use_sampling=True)\n",
        "            accuracy.append(ep_acc)\n",
        "            q_ep_acc = test_amortised_accuracy(model, test_img_list, test_label_list)\n",
        "            q_accuracy.append(q_ep_acc)\n",
        "            print(f\"Accuracy: {ep_acc}\")\n",
        "            print(f\"Amortised Accuracy: {q_ep_acc}\")\n",
        "            print(f\"Iterations: {info['n_iterations']}\")\n",
        "            np.save(\"accuracy.npy\" , np.array(accuracy))\n",
        "            np.save(\"q_accuracy.npy\" , np.array(q_accuracy))\n",
        "            np.save(\"n_iterations.npy\" , np.array(n_iterations))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Training Epoch 0\n",
            "Accuracy: 0.1859375\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 1\n",
            "Accuracy: 0.29296875\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 2\n",
            "Accuracy: 0.3515625\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 3\n",
            "Accuracy: 0.38671875\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 4\n",
            "Accuracy: 0.4125\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 5\n",
            "Accuracy: 0.3984375\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 6\n",
            "Accuracy: 0.42421875\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 7\n",
            "Accuracy: 0.43203125\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 8\n",
            "Accuracy: 0.40859375\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 9\n",
            "Accuracy: 0.43515625\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 10\n",
            "Accuracy: 0.43203125\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 11\n",
            "Accuracy: 0.4484375\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 12\n",
            "Accuracy: 0.4484375\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 13\n",
            "Accuracy: 0.45703125\n",
            "Amortised Accuracy: 0.11796875\n",
            "Iterations: 100\n",
            "> Training Epoch 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-830d736f1058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mep_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_img_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_infer_steps_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mq_ep_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_amortised_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_img_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-fcb2192a1b0a>\u001b[0m in \u001b[0;36mtest_accuracy\u001b[0;34m(model, x_list, y_list, n_infer_steps, use_sampling)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_infer_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclassification_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e7973cb44d03>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, n_steps, x_batch, y_batch, use_sampling)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamortised_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mn_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_iterations'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e7973cb44d03>\u001b[0m in \u001b[0;36mvariational_updates\u001b[0;34m(self, n_steps, x_batch, use_sampling)\u001b[0m\n\u001b[1;32m    142\u001b[0m                   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mmu_below\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                   \u001b[0mnew_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_momenta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPC_HMC_sample_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_below\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnew_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e2f4b080a9d7>\u001b[0m in \u001b[0;36mPC_HMC_sample_step\u001b[0;34m(self, prev_sample, pred_above, mu_below)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;31m#sample random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mmomenta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmass_metric_scalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mnew_samp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_momenta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymplectic_leapfrog_integrator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomenta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdFdmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrator_path_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrator_step_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0;31m#compute layer-wise free energies for MH acceptance step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mF_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_err\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihood_err\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlikelihood_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-e2e4efa3ba7f>\u001b[0m in \u001b[0;36msymplectic_leapfrog_integrator\u001b[0;34m(mus, momenta, dFdmu, path_len, step_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m#interweave updates for conservative symplectic dynamics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mmus\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmomenta\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mmomenta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdFdmu\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mmus\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmomenta\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}